{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rf_intro",
   "metadata": {},
   "source": [
    "# Random Forest Model Analysis\n",
    "### Analysis of Ezafe Classification Results\n",
    "*Author: Raha Musavi*\n",
    "\n",
    "*Matrikelnummer: 08022255354*\n",
    "\n",
    "*Thesis: Modeling the Presence and the Classification of Ezafe in Middle Persian Nominal Phrases*\n",
    "\n",
    "*Date: 01 May 2025*\n",
    "\n",
    "This notebook provides a detailed analysis of the trained Random Forest model for classifying Ezafe constructions in Middle Persian nominal phrases into four categories: 'No Ezafe & Head Initial', 'No Ezafe & Head Final', 'With Ezafe & Head Initial', and 'With Ezafe & Head Final'. It covers key evaluation metrics, Receiver Operating Characteristic (ROC) analysis, the learning curve, Partial Dependence Plots (PDPs), SHAP analysis, the random label shuffling test, and an analysis of misclassification errors. The structure and content of this notebook align with Chapter 6 of the thesis, providing the empirical results that support the discussion and conclusions.\n",
    "\n",
    "<span style=\"color: red;\">**Note on Model Configuration:**.</span> The analysis presented in this notebook utilizes the final trained Random Forest model (`random_forest_model-1.pkl`) that includes the preprocessed numerical features derived from the original `.conllu` corpus. This corresponds to the model configuration yielding the results presented in Chapter 6 of the thesis. The data used for analysis (`nonclausal-np_features.csv`) contains features extracted and preprocessed as described in Chapter 6, including label encoding for categorical features.\n",
    "\n",
    "This configuration, including the specific preprocessing steps applied to the data, was ultimately selected as the final model configuration for the comprehensive analysis presented in the thesis. Its strong performance and interpretability through feature importance and dependence analyses provide valuable insights into the linguistic factors influencing Ezafe classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup and Data Loading (for Analysis)\n",
    "\n",
    "# Import necessary libraries for data manipulation, modeling, evaluation, and visualization.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib # Used for loading the pre-trained model\n",
    "from sklearn.model_selection import train_test_split, learning_curve, StratifiedKFold # Needed for reproducing the split and calculating the learning curve\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score # Standard evaluation metrics\n",
    "from imblearn.over_sampling import RandomOverSampler # Used to reproduce the oversampling step\n",
    "from sklearn.utils import shuffle # Used for the random label shuffling test\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize # For encoding and binarizing labels\n",
    "from sklearn.inspection import PartialDependenceDisplay, permutation_importance # For model interpretability\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "import seaborn as sns # For enhanced visualizations (e.g., heatmaps)\n",
    "import os # To check for file existence\n",
    "\n",
    "# Configure plotting styles for consistency and readability\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Define a distinct colorblind-friendly palette for multi-class plots\n",
    "okabe_ito_palette = [\n",
    "    \"#E69F00\",  # orange\n",
    "    \"#56B4E9\",  # sky blue\n",
    "    \"#009E73\",  # bluish green\n",
    "    \"#F0E442\",  # yellow\n",
    "    \"#0072B2\",  # blue\n",
    "    \"#D55E00\",  # vermillion\n",
    "    \"#CC79A7\",  # reddish purple\n",
    "    \"#999999\",  # grey\n",
    "    \"#000000\",  # black\n",
    "    \"#A6761D\",  # brown\n",
    "    \"#1B9E77\",  # teal\n",
    "    \"#E7298A\",  # pink\n",
    "    \"#66A61E\",  # green\n",
    "    \"#7570B3\",  # purple\n",
    "    \"#E6AB02\",  # mustard\n",
    "    \"#A6CEE3\",  # pale blue\n",
    "    \"#B2DF8A\",  # pale green\n",
    "    \"#FB9A99\",  # pale red\n",
    "]\n",
    "custom_palette = sns.color_palette(okabe_ito_palette)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define input file paths\n",
    "INPUT_FEATURES_CSV = \"nonclausal-np_features.csv\" # Contains the features after preprocessing and extraction\n",
    "INPUT_MODEL_PATH = \"random_forest_model-1.pkl\" # The saved, trained Random Forest model\n",
    "\n",
    "# Configuration parameters needed to reproduce the specific train/test split and resampling performed during training.\n",
    "TEST_SIZE = 0.2 # Consistent with RF training notebook\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Configuration for the Learning Curve plot\n",
    "CV_FOLDS_LC = 5\n",
    "\n",
    "# Define human-readable label mapping for the combined target variable\n",
    "LABEL_MAPPING = {\n",
    "    \"0_1\": \"No Ezafe & Head Initial\",\n",
    "    \"0_2\": \"No Ezafe & Head Final\",\n",
    "    \"1_1\": \"With Ezafe & Head Initial\",\n",
    "    \"1_2\": \"With Ezafe & Head Final\"\n",
    "}\n",
    "\n",
    "# --- Load Data and Trained Model ---\n",
    "if not os.path.exists(INPUT_FEATURES_CSV) or not os.path.exists(INPUT_MODEL_PATH):\n",
    "    print(f\"Error: Required input files not found.\")\n",
    "    print(f\"Please ensure '{INPUT_FEATURES_CSV}' and '{INPUT_MODEL_PATH}' exist by running previous scripts.\")\n",
    "else:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(INPUT_FEATURES_CSV)\n",
    "\n",
    "    # Load the saved trained Random Forest model\n",
    "    best_model = joblib.load(INPUT_MODEL_PATH)\n",
    "\n",
    "    # --- Preprocessing for Analysis (Consistent with Training) ---\n",
    "    # Encode categorical features\n",
    "    categorical_cols = ['dependent_upos', 'dependent_deprel', 'nominal_head_deprel', \n",
    "                        'np_deprel_pattern', 'head_number', 'source_file', 'nominal_head_upos']\n",
    "    # Note: dependent_form, nominal_head_form were not used in the final model based on VIF analysis\n",
    "\n",
    "    label_encoders = {col: LabelEncoder() for col in categorical_cols if col in df.columns}\n",
    "\n",
    "    for col in label_encoders:\n",
    "        df[col] = df[col].astype(str).fillna(\"unknown\")\n",
    "        df[col] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "    # Create and encode the combined target variable (Ezafe + Position)\n",
    "    df['combined_label'] = df['ezafe_label'].astype(str) + \"_\" + df['position'].astype(str)\n",
    "    # LabelEncoder for the combined target\n",
    "    le_combined = LabelEncoder()\n",
    "    df['combined_label'] = le_combined.fit_transform(df['combined_label'])\n",
    "\n",
    "    # Drop columns not used in the final model based on VIF analysis and targets\n",
    "    # Ensure this drop list matches the features the loaded model was trained on.\n",
    "    drop_cols_final_model = [\n",
    "        'nominal_head_id', 'nominal_head_form', 'dependent_id', 'dependent_form', \n",
    "        'ezafe_label', 'position', 'combined_label' \n",
    "        # VIF drop features already implicitly handled by the model's feature_names_in_\n",
    "    ]\n",
    "\n",
    "    X = df.drop(columns=drop_cols_final_model, errors='ignore')\n",
    "    y = df['combined_label']\n",
    "\n",
    "    # Ensure all features are numeric (should be after encoding)\n",
    "    X = X.apply(pd.to_numeric, errors='coerce').fillna(-1)\n",
    "\n",
    "    # Ensure X columns match the order expected by the model\n",
    "    feature_order = list(best_model.feature_names_in_)\n",
    "    X = X.reindex(columns=feature_order, fill_value=0)\n",
    "\n",
    "    # Handle class imbalance by oversampling (reproduce training oversampling)\n",
    "    ros = RandomOverSampler(sampling_strategy='auto', random_state=RANDOM_STATE)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "    # Split the resampled data into training and test sets (reproduce training split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_resampled, y_resampled,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_resampled \n",
    "    )\n",
    "\n",
    "    print(f\"Loaded data shape (before resampling): {df.shape}\")\n",
    "    print(f\"Loaded trained Random Forest model.\")\n",
    "    print(f\"Reproduced resampled data shape (for analysis): {X_resampled.shape}\")\n",
    "    print(f\"Reproduced Train data shape: {X_train.shape}\")\n",
    "    print(f\"Reproduced Test data shape: {X_test.shape}\")\n",
    "\n",
    "    # Store decoded class names for later use in plotting/reporting\n",
    "    decoded_class_labels = le_combined.inverse_transform(best_model.classes_)\n",
    "    human_readable_class_names = [LABEL_MAPPING.get(str(label), str(label)) for label in decoded_class_labels]\n",
    "    # Ensure the order of class names matches the model's classes_ attribute for correct confusion matrix plotting\n",
    "    class_names_ordered = [LABEL_MAPPING.get(str(label), str(label)) for label in best_model.classes_]\n",
    "    \n",
    "else:\n",
    "    print(\"Required files not found. Analysis cannot proceed.\")\n",
    "    # Exit or handle the error appropriately\n",
    "    # sys.exit() # If you want to stop script execution\n",
    "\n",
    "# Continue with analysis sections only if data and model loaded successfully\n",
    "analysis_possible = 'best_model' in locals() and 'X_test' in locals()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_results_overall",
   "metadata": {},
   "source": [
    "## 6.9 Results\n",
    "\n",
    "This section presents the primary evaluation metrics for the Random Forest model on the test set (`X_test`, `y_test`), as discussed in Section 6.9. These metrics provide an overall assessment of the model's performance in classifying Ezafe constructions into the four predefined categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_overall_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Make predictions on the test set using the loaded model.\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    print(\"--- Test Set Evaluation (Section 6.9) ---\")\n",
    "    # Calculate and print the overall accuracy.\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    # Print the detailed classification report, including precision, recall, and f1-score for each class.\n",
    "    # Use the ordered human-readable class names for the target_names argument.\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=class_names_ordered, zero_division='warn'))\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_confusion_matrix_md",
   "metadata": {},
   "source": [
    "### 6.9.1 Confusion Matrix\n",
    "\n",
    "The confusion matrix visualizes the performance of the multi-class classification model. Each row represents the instances in a true class, while each column represents the instances in a predicted class. It shows the counts of correctly and incorrectly classified instances for each of the four Ezafe construction categories on the test set (Section 6.9.1, Figure 23)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_confusion_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Ensure y_pred is calculated from the previous cell\n",
    "    if 'y_pred' not in locals():\n",
    "         y_pred = best_model.predict(X_test)\n",
    "         \n",
    "    # Calculate the confusion matrix array.\n",
    "    # Use the model's classes_ attribute to ensure correct ordering of rows/columns\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=best_model.classes_)\n",
    "\n",
    "    # Create the heatmap for visualization (Section 6.9.1, Figure 23 equivalent).\n",
    "    # Use the ordered human-readable class names for axis labels.\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        annot=True, # Display counts on the cells.\n",
    "        fmt='d', # Format annotations as integers.\n",
    "        cmap='Blues', # Colormap.\n",
    "        xticklabels=class_names_ordered, # Labels for predicted classes.\n",
    "        yticklabels=class_names_ordered  # Labels for true classes.\n",
    "    )\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix for the Random Forest Model\")\n",
    "    plt.tight_layout()\n",
    "    # Save the figure (optional)\n",
    "    # plt.savefig(\"rf_confusion_matrix.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_evaluation_intro",
   "metadata": {},
   "source": [
    "## 6.10 Evaluation of the Model\n",
    "\n",
    "This section provides a more in-depth evaluation of the Random Forest model's performance and generalization capabilities using various metrics and visualizations, as discussed in Section 6.10. This includes Feature Importance (Gini and Permutation), Partial Dependence Plots (PDPs), SHAP analysis, the learning curve, and the random label shuffling test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_feature_importance_md",
   "metadata": {},
   "source": [
    "### 6.10.1 Feature Importance\n",
    "\n",
    "Feature importance measures how much each feature contributes to the model's predictions. In Random Forests, Gini importance (calculated from the reduction in impurity achieved by splits on a feature) provides an initial estimate (Section 6.10.1, Figure 19). Permutation importance, a more robust method, measures the decrease in model performance when a feature's values are randomly shuffled, breaking its relationship with the target (Section 6.10.1, Figure 20). Both methods help identify the most influential linguistic features in classifying Ezafe constructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_gini_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Extract Gini feature importances directly from the trained model (Section 6.10.1, Figure 19 equivalent).\n",
    "    importances = best_model.feature_importances_\n",
    "\n",
    "    # Create a DataFrame to display and plot the importances.\n",
    "    feature_names = X_test.columns # Use test set columns as feature names\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot the Gini feature importances.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Use a palette from the custom_palette based on the number of features\n",
    "    sns.barplot(\n",
    "        data=feature_importance_df,\n",
    "        x='Importance',\n",
    "        y='Feature',\n",
    "        palette=sns.color_palette('viridis', n_colors=len(feature_importance_df)), # Use viridis for Gini\n",
    "        orient='h'\n",
    "    )\n",
    "    plt.title('Gini Feature Importances for the Random Forest Model')\n",
    "    plt.tight_layout()\n",
    "    # Save the figure (optional)\n",
    "    # plt.savefig(\"rf_gini_importance.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Gini Feature Importances (Section 6.10.1, Figure 19 equivalent):\")\n",
    "    print(feature_importance_df.to_string())\n",
    "\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_permutation_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Calculate Permutation Feature Importances (Section 6.10.1, Figure 20 equivalent).\n",
    "    # This is computed on the test set to measure the impact on unseen data.\n",
    "    print(\"Calculating permutation importances...\")\n",
    "    result = permutation_importance(\n",
    "        best_model,\n",
    "        X_test, y_test,\n",
    "        n_repeats=10, # Number of times to shuffle and score - using 10 for quicker run, 30+ recommended for final analysis\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1 # Use all available CPU cores\n",
    "    )\n",
    "\n",
    "    # Create a DataFrame to display and plot the permutation importances.\n",
    "    # Use the feature names from the X_test DataFrame columns.\n",
    "    permutation_importance_df = pd.DataFrame({\n",
    "        'Feature': X_test.columns,\n",
    "        'Importance Mean': result.importances_mean,\n",
    "        'Importance Std': result.importances_std\n",
    "    }).sort_values(by='Importance Mean', ascending=False)\n",
    "\n",
    "    # Plot the Permutation Feature Importances.\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Select top features for better visualization - adjust 'top_n' as needed\n",
    "    top_n_features = 16\n",
    "    top_permutation_df = permutation_importance_df.head(top_n_features)\n",
    "\n",
    "    # Use a palette from the custom_palette based on the number of top features\n",
    "    plt.barh(\n",
    "        top_permutation_df['Feature'],\n",
    "        top_permutation_df['Importance Mean'],\n",
    "        xerr=top_permutation_df['Importance Std'], # Error bars represent standard deviation\n",
    "        color=custom_palette[:top_n_features], # Use the custom palette\n",
    "        align='center'\n",
    "    )\n",
    "    plt.xlabel('Mean Decrease in Accuracy (Permutation)')\n",
    "    plt.title(f'Permutation Feature Importance (Top {top_n_features} Features)')\n",
    "    plt.gca().invert_yaxis() # Invert y-axis to show the most important features at the top\n",
    "    plt.tight_layout()\n",
    "    # Save the figure (optional)\n",
    "    # plt.savefig(\"rf_permutation_importance.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Permutation Feature Importances (Section 6.10.1, Figure 20 equivalent):\")\n",
    "    print(permutation_importance_df.to_string())\n",
    "\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_pdp_md",
   "metadata": {},
   "source": [
    "### 6.10.2 Partial Dependence Plots (PDPs)\n",
    "\n",
    "Partial Dependence Plots (PDPs) show the marginal effect of one or two features on the predicted outcome of a machine learning model. They illustrate how the model's prediction changes as the value(s) of the feature(s) change, averaged over the values of other features. PDPs help visualize the relationship between specific features and the model's output and can reveal non-linear patterns (Section 6.10.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_pdp_distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Generate a PDP for the 'distance' feature (Section 6.10.2, Figure 27 equivalent).\n",
    "    # This plot shows the average effect of the distance between the head and the dependent on the model's prediction.\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        best_model, \n",
    "        X_test, # Use the test set features for calculation\n",
    "        features=['distance'], # Specify the feature to plot\n",
    "        ax=ax # Use the created axes\n",
    "    )\n",
    "    plt.suptitle(\"Partial Dependence Plot for the Effect of Distance\")\n",
    "    plt.tight_layout()\n",
    "    # Save the figure (optional)\n",
    "    # plt.savefig(\"rf_pdp_distance.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_pdp_num_dependents_dependent",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Generate a PDP for the 'num_dependents_dependent' feature (Section 6.10.2, Figure 28 equivalent).\n",
    "    # This plot shows the average effect of the number of dependents of the dependent on the model's prediction.\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        best_model, \n",
    "        X_test, # Use the test set features for calculation\n",
    "        features=['num_dependents_dependent'], # Specify the feature to plot\n",
    "        ax=ax # Use the created axes\n",
    "    )\n",
    "    plt.suptitle(\"Partial Dependence Plot for the Effect of Number of Dependent's Dependents\")\n",
    "    plt.tight_layout()\n",
    "    # Save the figure (optional)\n",
    "    # plt.savefig(\"rf_pdp_num_dependents_dependent.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_pdp_num_dependents_head",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Generate a PDP for the 'num_dependents_head' feature (Section 6.10.2, Figure 29 equivalent).\n",
    "    # This plot shows the average effect of the number of dependents of the head on the model's prediction.\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        best_model, \n",
    "        X_test, # Use the test set features for calculation\n",
    "        features=['num_dependents_head'], # Specify the feature to plot\n",
    "        ax=ax # Use the created axes\n",
    "    )\n",
    "    plt.suptitle(\"Partial Dependence Plot for the Effect of Number of Head's Dependents\")\n",
    "    plt.tight_layout()\n",
    "    # Save the figure (optional)\n",
    "    # plt.savefig(\"rf_pdp_num_dependents_head.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_pdp_np_depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Generate a PDP for the 'np_depth' feature (Section 6.10.2, Figure 30 equivalent).\n",
    "    # This plot shows the average effect of the nominal phrase depth on the model's prediction.\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        best_model, \n",
    "        X_test, # Use the test set features for calculation\n",
    "        features=['np_depth'], # Specify the feature to plot\n",
    "        ax=ax # Use the created axes\n",
    "    )\n",
    "    plt.suptitle(\"Partial Dependence Plot for the Effect of Nominal Phrase Depth\")\n",
    "    plt.tight_layout()\n",
    "    # Save the figure (optional)\n",
    "    # plt.savefig(\"rf_pdp_np_depth.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_shap_md",
   "metadata": {},
   "source": [
    "### 6.10.3 SHAP Analysis\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values provide a powerful framework for interpreting individual predictions and understanding global feature importance by attributing the contribution of each feature to the model's output for each instance. For tree-based models like Random Forests, TreeExplainer can be used to efficiently calculate SHAP values (Section 6.10.3).\n",
    "\n",
    "SHAP summary plots (Section 6.10.3, Figure 21) provide a global view of feature importance:\n",
    "- The **bar plot** shows the average absolute SHAP value for each feature across all instances. This is a global measure of feature importance, similar to Gini or permutation importance but based on SHAP values.\n",
    "- The **beeswarm plot** shows the distribution of SHAP values for each feature across all instances. Each dot represents a single instance's SHAP value for that feature. The color indicates the feature value (e.g., red high, blue low). This plot reveals how a feature's value affects the prediction (e.g., high values consistently increasing/decreasing the prediction) and the distribution of effects. For multi-class classification, SHAP values are typically calculated for each class, showing how features influence the prediction *for that specific class*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_shap",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Calculate SHAP values for the Random Forest model (Section 6.10.3, Figure 21 equivalent).\n",
    "    # Using the TreeExplainer is efficient for tree-based models.\n",
    "    # Note: SHAP calculation can be computationally intensive for large datasets.\n",
    "    # You might consider using a subset of X_test (e.g., shap.utils.sample) if it takes too long.\n",
    "\n",
    "    # Ensure the SHAP library is installed (`pip install shap`)\n",
    "    try:\n",
    "        import shap\n",
    "    except ImportError:\n",
    "        print(\"Please install the shap library: pip install shap\")\n",
    "        analysis_possible = False # Prevent further SHAP analysis if import fails\n",
    "\n",
    "if analysis_possible:\n",
    "    # Create a SHAP TreeExplainer with the trained Random Forest model.\n",
    "    explainer = shap.TreeExplainer(best_model, X_test) # Using X_test as the background dataset\n",
    "\n",
    "    # Calculate SHAP values for the test set.\n",
    "    # shap_values will be a list of arrays, one array for each class.\n",
    "    # Each array has shape (n_samples, n_features).\n",
    "    print(\"Calculating SHAP values...\")\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    print(\"SHAP value calculation complete.\")\n",
    "\n",
    "    # Plot SHAP summary plots for each class (Section 6.10.3, Figure 21 equivalent).\n",
    "    # Iterate through each class calculated by the model.\n",
    "    # model.classes_ contains the encoded numerical labels (0, 1, 2, 3).\n",
    "    for i, encoded_class_label in enumerate(best_model.classes_):\n",
    "        # Get the human-readable class name using the mapping created in the setup cell.\n",
    "        human_label = LABEL_MAPPING.get(str(le_combined.inverse_transform([encoded_class_label])[0]), str(encoded_class_label))\n",
    "        \n",
    "        print(f\"\\nðŸ“Œ SHAP Summary Plots for class: {human_label}\")\n",
    "\n",
    "        # SHAP Bar Plot: Average absolute SHAP values for features for this class.\n",
    "        # This shows feature importance for predicting THIS specific class.\n",
    "        shap.summary_plot(shap_values[i], X_test, plot_type=\"bar\", show=False) # Use show=False to control plotting with plt.show()\n",
    "        plt.title(f\"SHAP Feature Importance (Average Absolute SHAP) for Class: {human_label}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # SHAP Beeswarm Plot: Distribution of SHAP values for features for this class across instances.\n",
    "        # This shows how different feature values impact the prediction for this class.\n",
    "        shap.summary_plot(shap_values[i], X_test, show=False) # Use show=False to control plotting with plt.show()\n",
    "        plt.title(f\"SHAP Value Distribution for Class: {human_label}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup or SHAP import.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_shap_groups_md",
   "metadata": {},
   "source": [
    "#### SHAP Values by Feature Group\n",
    "\n",
    "Aggregating SHAP values by feature groups (e.g., structural, morphosyntactic, lexical) provides a higher-level understanding of which broad categories of linguistic information are most influential in the model's predictions for each class (Section 6.10.3). This helps connect the model's learned patterns back to linguistic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_shap_groups",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible and 'shap_values' in locals():\n",
    "    # Define feature groups based on the columns present in X_test.\n",
    "    # These groups should align with the linguistic categories discussed in the thesis.\n",
    "    feature_groups = {\n",
    "        \"Source Info\": [col for col in X_test.columns if col.startswith('source_file_')],\n",
    "        \"Morphosyntactic\": [\n",
    "            'nominal_head_upos', 'nominal_head_deprel', 'head_number',\n",
    "            'dependent_upos', 'dependent_deprel', 'np_deprel_pattern'\n",
    "        ],\n",
    "        \"Structural & Complexity\": [\n",
    "            'distance', 'num_nouns_before', 'num_adjs_before', 'num_adjs_after',\n",
    "            'np_depth', 'num_dependents_head', 'num_dependents_dependent', \n",
    "            'relative_position_in_sent' # Adding relative_position_in_sent to structural/complexity group\n",
    "        ],\n",
    "        \"Lexical & Frequency\": [\n",
    "             'head_frequency', 'dependent_frequency', 'modifier_to_head_ratio'\n",
    "        ],\n",
    "        \"Dependent Type\": [col for col in X_test.columns if col.startswith('dependent_upos_') and col != 'dependent_upos'],\n",
    "        \"Dependent Deprel Detail\": [col for col in X_test.columns if col.startswith('dependent_deprel_') and col != 'dependent_deprel'],\n",
    "        \"Head Form\": [col for col in X_test.columns if col.startswith('nominal_head_form_')],\n",
    "        \"Dependent Form\": [col for col in X_test.columns if col.startswith('dependent_form_')],\n",
    "        \"Is Verbal\": ['is_verbal']\n",
    "    }\n",
    "    \n",
    "    # --- Handle potential missing features in groups --- \n",
    "    # Ensure that each feature name in the groups actually exists in X_test.columns\n",
    "    # This is important if some OHE columns were dropped due to low frequency or VIF.\n",
    "    cleaned_feature_groups = {}\n",
    "    all_features_in_groups = set()\n",
    "    for group_name, features in feature_groups.items():\n",
    "        cleaned_features = [f for f in features if f in X_test.columns]\n",
    "        cleaned_feature_groups[group_name] = cleaned_features\n",
    "        all_features_in_groups.update(cleaned_features)\n",
    "\n",
    "    # Identify features in X_test but not in any defined group (should be none if groups are comprehensive)\n",
    "    features_not_in_groups = set(X_test.columns) - all_features_in_groups\n",
    "    if features_not_in_groups:\n",
    "        print(f\"Warning: The following features are in X_test but not in any defined group: {features_not_in_groups}\")\n",
    "    \n",
    "    feature_groups = cleaned_feature_groups # Use the cleaned groups\n",
    "\n",
    "    # --- Calculate Mean Absolute SHAP per Group per Class ---\n",
    "    group_shap_summary = {}\n",
    "\n",
    "    for i, encoded_class_label in enumerate(best_model.classes_):\n",
    "        human_label = LABEL_MAPPING.get(str(le_combined.inverse_transform([encoded_class_label])[0]), str(encoded_class_label))\n",
    "        class_shap_values = shap_values[i] # SHAP values for this class\n",
    "\n",
    "        group_shap = {}\n",
    "        total_abs_shap_class = np.abs(class_shap_values).mean() # Overall mean abs SHAP for this class\n",
    "\n",
    "        if total_abs_shap_class == 0:\n",
    "             print(f\"Warning: Total absolute SHAP for class {human_label} is zero.\")\n",
    "             for group_name in feature_groups.keys():\n",
    "                 group_shap[group_name] = 0\n",
    "        else:\n",
    "            for group_name, features in feature_groups.items():\n",
    "                if features: # Only calculate if the group has features present in X_test\n",
    "                    # Get SHAP values for features in this group and calculate the mean absolute value\n",
    "                    group_shap_values = class_shap_values[:, X_test.columns.get_indexer(features)]\n",
    "                    mean_abs_shap_group = np.abs(group_shap_values).mean()\n",
    "                    # Store as percentage of the total absolute SHAP for the class\n",
    "                    group_shap[group_name] = (mean_abs_shap_group / total_abs_shap_class) * 100\n",
    "                else:\n",
    "                    group_shap[group_name] = 0 # Group has no features in X_test\n",
    "                    \n",
    "        group_shap_summary[human_label] = group_shap\n",
    "\n",
    "    # Create a DataFrame for the grouped SHAP summary.\n",
    "    group_shap_df = pd.DataFrame(group_shap_summary).T # Transpose to have groups as index and classes as columns\n",
    "    group_shap_df = group_shap_df.round(2) # Round for display clarity\n",
    "\n",
    "    # Display the grouped SHAP summary table.\n",
    "    print(\"\\nGrouped SHAP Contributions (Percentage of Mean Absolute SHAP per Class):\")\n",
    "    print(group_shap_df.to_string())\n",
    "\n",
    "    # Save the grouped SHAP summary to a CSV file (optional).\n",
    "    # group_shap_df.to_csv(\"rf_grouped_shap_contributions.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup or SHAP values availability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_learning_curve_md",
   "metadata": {},
   "source": [
    "### 6.10.4 Learning Curve\n",
    "\n",
    "The learning curve plots the model's performance on the training and validation sets against the number of training samples. It helps diagnose bias and variance issues. A converging curve with a small gap indicates good generalization, while a large gap suggests overfitting. Low scores on both curves might indicate underfitting (Section 6.10.4, Figure 26)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_learning_curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Calculate learning curve data (Section 6.10.4, Figure 26 equivalent).\n",
    "    # Use the resampled dataset (X_resampled, y_resampled) as the data source for the learning curve.\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        best_model, # The trained model object to evaluate.\n",
    "        X_resampled, y_resampled, # The full dataset after oversampling.\n",
    "        cv=CV_FOLDS_LC, # Number of cross-validation folds for the learning curve calculation.\n",
    "        scoring='accuracy', # Evaluation metric.\n",
    "        n_jobs=-1, # Use all available cores.\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10), # Evaluate at 10 different training set sizes.\n",
    "        random_state=RANDOM_STATE # Seed for reproducibility.\n",
    "    )\n",
    "\n",
    "    # Calculate the mean and standard deviation of the training and test scores across the folds.\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot the learning curve (Section 6.10.4, Figure 26 equivalent).\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(train_sizes, train_mean, label='Training score')\n",
    "    plt.plot(train_sizes, test_mean, label='Validation score')\n",
    "\n",
    "    # Plot the shaded area representing variability.\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "\n",
    "    plt.xlabel('Training Samples')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Learning Curve for the Random Forest Model')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_random_shuffle_md",
   "metadata": {},
   "source": [
    "### 6.10.5 Random Label Shuffling\n",
    "\n",
    "The random label shuffling test assesses if the model is learning meaningful patterns or merely memorizing noise. By randomly shuffling the target labels in the training set, we break the true feature-label relationship. A model learning real patterns should perform poorly (near random chance) on this data. Conversely, high accuracy on shuffled data indicates the model is likely overfitting or learning from data artifacts (Section 6.10.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_random_shuffle",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Perform the random label shuffling test (Section 6.10.5).\n",
    "    # Shuffle the training labels (y_train) while keeping X_train in its original order.\n",
    "    y_train_shuffled = shuffle(y_train, random_state=RANDOM_STATE) # Use the same random state for reproducibility.\n",
    "\n",
    "    # Train a new Random Forest model on the shuffled data using the best hyperparameters.\n",
    "    # We use the same model type and best hyperparameters as the primary model.\n",
    "    shuffled_model = RandomForestClassifier(\n",
    "        n_estimators=best_model.n_estimators, \n",
    "        max_depth=best_model.max_depth, \n",
    "        min_samples_split=best_model.min_samples_split, \n",
    "        class_weight=best_model.class_weight,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    print(\"Training a Random Forest model on shuffled labels...\")\n",
    "    shuffled_model.fit(X_train, y_train_shuffled)\n",
    "\n",
    "    # Evaluate the model trained on shuffled data using the *original* test set features (X_test) and *original* labels (y_test).\n",
    "    # The comparison to original labels shows how well the model generalizes when trained on noise.\n",
    "    y_pred_shuffled = shuffled_model.predict(X_test)\n",
    "    accuracy_shuffled = accuracy_score(y_test, y_pred_shuffled)\n",
    "\n",
    "    print(f\"Accuracy with shuffled training labels: {accuracy_shuffled:.4f}\")\n",
    "\n",
    "    # Compare this accuracy to the accuracy of the original best model on the original test set.\n",
    "    # This comparison is key to interpreting the test result.\n",
    "    if 'y_pred' not in locals():\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    accuracy_true = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with original training labels (from best_model): {accuracy_true:.4f}\")\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_error_analysis_md",
   "metadata": {},
   "source": [
    "### 6.10.6 Error Analysis\n",
    "\n",
    "Error analysis focuses on instances where the model's predictions differ from the true labels (misclassifications). By examining the feature values in these misclassified cases, we can gain insight into the types of linguistic contexts or feature patterns that challenge the model. Calculating the mean feature values across misclassified instances and their percentage contribution to the total misclassification helps pinpoint features associated with prediction errors (Section 6.10.6, Table 21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_error_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_possible:\n",
    "    # Ensure y_pred is calculated from the overall evaluation cell\n",
    "    if 'y_pred' not in locals():\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Identify misclassified instances in the test set (Section 6.10.6).\n",
    "    # Create a DataFrame of test set results including features for analysis.\n",
    "    # Ensure X_test index is reset for safe concatenation with y_test/y_pred Series.\n",
    "    X_test_reset = X_test.reset_index(drop=True)\n",
    "    y_test_series = pd.Series(y_test).reset_index(drop=True)\n",
    "    y_pred_series = pd.Series(y_pred).reset_index(drop=True)\n",
    "\n",
    "    test_results_df = pd.concat([X_test_reset, y_test_series.rename('Actual'), y_pred_series.rename('Predicted')], axis=1)\n",
    "\n",
    "    # Filter for misclassified instances.\n",
    "    misclassified_df = test_results_df[test_results_df['Actual'] != test_results_df['Predicted']].copy()\n",
    "\n",
    "    print(f\"Number of misclassified instances in the test set: {len(misclassified_df)}\")\n",
    "\n",
    "    # Exclude the 'Actual' and 'Predicted' columns before calculating the mean feature values.\n",
    "    feature_columns_in_error_df = misclassified_df.drop(columns=['Actual', 'Predicted']).columns\n",
    "\n",
    "    # Calculate the mean feature values for the misclassified cases (Section 6.10.6).\n",
    "    # This provides the average feature profile of cases where the model was wrong.\n",
    "    if not misclassified_df.empty:\n",
    "        misclassified_features_mean = misclassified_df[feature_columns_in_error_df].mean()\n",
    "\n",
    "        # Calculate the percentage contribution of each feature to misclassification (Section 6.10.6, Table 21 equivalent).\n",
    "        # This is based on the absolute mean values of features in misclassified cases.\n",
    "        total_abs_mean = misclassified_features_mean.abs().sum()\n",
    "\n",
    "        if total_abs_mean == 0:\n",
    "             print(\"Cannot calculate percentage contribution: Sum of absolute mean feature values is zero.\")\n",
    "             misclassified_features_percent = pd.Series(0, index=misclassified_features_mean.index)\n",
    "        else:\n",
    "            misclassified_features_percent = (misclassified_features_mean.abs() / total_abs_mean) * 100\n",
    "            # Re-normalize to ensure it sums to 100% due to potential floating point errors\n",
    "            misclassified_features_percent = (misclassified_features_percent / misclassified_features_percent.sum()) * 100\n",
    "\n",
    "        # Create a DataFrame summarizing the error analysis by feature.\n",
    "        misclassification_analysis_df = pd.DataFrame({\n",
    "            \"Mean Feature Value (Misclassified)\": misclassified_features_mean,\n",
    "            \"Percentage Contribution (%)\": misclassified_features_percent\n",
    "        })\n",
    "\n",
    "        # Sort by percentage contribution descending.\n",
    "        misclassification_analysis_df_sorted = misclassification_analysis_df.reindex(misclassified_features_percent.sort_values(ascending=False).index)\n",
    "\n",
    "        print(\"\\nFeatures Associated with Misclassification (Section 6.10.6, Table 21 equivalent):\")\n",
    "        # Display the sorted table.\n",
    "        print(misclassification_analysis_df_sorted.to_string())\n",
    "    else:\n",
    "        print(\"No misclassified instances to analyze feature contributions.\")\n",
    "\n",
    "else:\n",
    "    print(\"Analysis not possible. Please check setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_summary_conclusion",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "This notebook provides a comprehensive analysis of the Random Forest model trained for multi-class classification of Ezafe constructions, as detailed in Chapter 6 of the thesis. Leveraging various evaluation and interpretability techniques, the analysis aimed to understand the model's performance, learned patterns, and areas of difficulty.\n",
    "\n",
    "The Random Forest model demonstrated high overall accuracy (referencing output from Cell 5) and strong per-class performance (referencing outputs from Cell 6), including high precision and recall for the 'With Ezafe' classes and solid performance for the 'No Ezafe' classes, despite the initial class imbalance addressed through oversampling.\n",
    "\n",
    "Key insights from feature importance (Gini and Permutation, referencing outputs from Cells 8 and 9) and Partial Dependence Plots (referencing outputs from Cells 11, 12, 13, 14) highlight the critical role of structural and complexity features. 'Distance' and 'Number of dependents of the dependent' were consistently identified as highly influential, indicating the model's sensitivity to the syntactic environment of the head-dependent pair. The relationship between these features and predictions, visualized through PDPs, revealed non-linear patterns, suggesting complex linguistic conditioning.\n",
    "\n",
    "SHAP analysis provided a deeper understanding of feature contributions for each class (referencing outputs from Cells 15 and 16). While structural features were globally important, SHAP values showed how different feature groups influence the prediction of specific Ezafe construction types. Lexical and morphosyntactic features, although less prominent overall, still played roles in distinguishing certain classes.\n",
    "\n",
    "Evaluation procedures, including the learning curve (referencing output from Cell 17) and the random label shuffling test (referencing output from Cell 18), confirmed the model's ability to generalize beyond the training data and learn from genuine patterns rather than noise.\n",
    "\n",
    "Error analysis (referencing output from Cell 20) provided valuable insight into the types of instances that the model misclassified. Features with high variability or complex interactions (like dependency patterns) and certain lexical features were found to be associated with prediction errors, suggesting areas where linguistic patterns might be less clear-cut or harder for the model to generalize.\n",
    "\n",
    "The findings from the Random Forest analysis, when compared with those from the Logistic Regression model (as discussed in Chapter 7 of the thesis), provide a comprehensive view of the predictive power of the selected linguistic features. The Random Forest model's superior performance, particularly in handling the nuances of different Ezafe construction types, suggests that it effectively captures complex, non-linear interactions within the linguistic features. This analysis underscores the potential of computational approaches to uncover subtle patterns in historical linguistic data and contributes to a data-driven understanding of Ezafe syntax in Middle Persian.\n",
    "\n",
    "It is noted that the specific numerical results obtained in this notebook execution may show minor variations compared to figures and tables presented in the thesis (Chapter 6). As discussed previously (referencing Cell 2), these differences are attributed to the dynamic nature of computational environments and potential minor variations in internal random states or library versions. However, the methodology implemented in the code is consistent with that described in the thesis, and the overall patterns and conclusions drawn regarding the model's performance and linguistic insights are aligned with the thesis findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_note",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
